{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating An Article Summerizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This text summarization project on Wikipedia data is a powerful model that extracts key information from lengthy Wikipedia articles and generates concise summaries. Using advanced natural language processing techniques, our project analyzes the content of Wikipedia articles, identifies important sentences, and condenses them into a coherent and informative summary. With our text summarization model, users can easily obtain the most relevant information from Wikipedia articles, saving time and effort when researching or exploring various topics. Experience the efficiency and effectiveness of this text summarization project to quickly grasp the essence of extensive Wikipedia content. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\freta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\freta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import bs4 as bs \n",
    "import urllib.request\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To deal with the  [SSL: CERTIFICATE_VERIFY_FAILED] issue \n",
    "import ssl\n",
    "context = ssl._create_unverified_context()\n",
    "\n",
    "# Get the data\n",
    "source = urllib.request.urlopen('https://en.wikipedia.org/wiki/Global_warming', context=context).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = bs.BeautifulSoup(source, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the text paragraph from the the html\n",
    "gw_text = \"\"\n",
    "\n",
    "for paragraph in soup.find_all('p'):\n",
    "    gw_text += paragraph.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gw_text_ed = re.sub(r'\\[[0-9]*\\]', ' ', gw_text)\n",
    "\n",
    "gw_text_ed = re.sub(r'\\s+', ' ', gw_text_ed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create another clean text data that can be used to create a histogram. \n",
    "# let us remove all the non-word characters, digits, and extra spaces.\n",
    "\n",
    "clean_text = gw_text_ed.lower()\n",
    "clean_text = re.sub(r'\\W', ' ',  clean_text)\n",
    "clean_text = re.sub(r'\\d', ' ', clean_text)\n",
    "clean_text = re.sub(r'\\s+', ' ', clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(gw_text_ed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2count = {}\n",
    "for word in nltk.word_tokenize(clean_text):\n",
    "    if word not in stop_words:\n",
    "        if word not in word2count.keys():\n",
    "            word2count[word] = 1\n",
    "        else:\n",
    "            word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in word2count.keys():\n",
    "    word2count[key] = word2count[key]/max(word2count.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the Sentence Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent2score = {}\n",
    "for sentence in sentences:\n",
    "    for word in nltk.word_tokenize(sentence.lower()):\n",
    "        if word in word2count.keys():\n",
    "            if len(sentence.split(' ')) <25:\n",
    "                if sentence not in sent2score.keys():\n",
    "                    sent2score[sentence] = word2count[word]\n",
    "                else:\n",
    "                    sent2score[sentence] += word2count[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best sentences\n",
    "\n",
    "best_sentences = heapq.nlargest(5, sent2score, key=sent2score.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activists also initiate lawsuits which target governments and demand that they take ambitious action or enforce existing laws on climate change.\n",
      "***************************\n",
      "This is then used as input for physical climate models and carbon cycle models to predict how atmospheric concentrations of greenhouse gases might change.\n",
      "***************************\n",
      "Policy decisions that rely on carbon dioxide removal increase the risk of global warming rising beyond international goals.\n",
      "***************************\n",
      "Per-capita emissions were also still relatively low in developing countries and developing countries would need to emit more to meet their development needs.\n",
      "***************************\n",
      "Major increases in energy efficiency investment will be required to achieve climate goals, comparable to the level of investment in renewable energy.\n",
      "***************************\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for bs in best_sentences:\n",
    "    print(bs)\n",
    "    print('***************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
